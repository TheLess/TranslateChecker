"""
LLMç¿»è¯‘è´¨é‡è¯„ä¼°å™¨
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦çš„ç¿»è¯‘è´¨é‡åˆ†æ
"""

import json
import re
from typing import Dict, List, Any, Optional
import pandas as pd
import openai
from tqdm import tqdm
import time

from ..utils.config_loader import ConfigLoader
from ..utils.logger import get_logger


class LLMEvaluator:
    """LLMç¿»è¯‘è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–LLMè¯„ä¼°å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = ConfigLoader(config_path)
        self.logger = get_logger(__name__)
        
        # è·å–LLMé…ç½®
        self.llm_config = self.config.get('llm', {})
        self.provider = self.llm_config.get('provider', 'openai')
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_client()
        
        # è·å–æç¤ºè¯æ¨¡æ¿
        self.prompt_template = self.llm_config.get('prompt_template', self._get_default_prompt())
        
    def _init_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        if self.provider == 'openai':
            self._init_openai_client()
        elif self.provider == 'ollama':
            self._init_ollama_client()
        elif self.provider == 'qwen':
            self._init_qwen_client()
        elif self.provider == 'glm':
            self._init_glm_client()
        elif self.provider == 'wenxin':
            self._init_wenxin_client()
        else:
            supported_providers = ['openai', 'ollama', 'qwen', 'glm', 'wenxin']
            self.logger.warning(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {self.provider}")
            self.logger.info(f"æ”¯æŒçš„æä¾›å•†: {', '.join(supported_providers)}")
            self.client = None
            
    def _init_openai_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
            api_key = self.llm_config.get('openai', {}).get('api_key')
            if not api_key:
                import os
                api_key = os.getenv('OPENAI_API_KEY')
                
            if not api_key:
                self.logger.warning("æœªæ‰¾åˆ°OpenAI APIå¯†é’¥ï¼ŒLLMè¯„ä¼°åŠŸèƒ½å°†ä¸å¯ç”¨")
                self.client = None
                return
                
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            self.client = openai.OpenAI(api_key=api_key)
            self.model_name = self.llm_config.get('openai', {}).get('model', 'gpt-3.5-turbo')
            self.max_tokens = self.llm_config.get('openai', {}).get('max_tokens', 1000)
            self.temperature = self.llm_config.get('openai', {}).get('temperature', 0.1)
            
            self.logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
    
    def _init_ollama_client(self):
        """åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯"""
        try:
            import requests
            
            # è·å–Ollamaé…ç½®
            ollama_config = self.llm_config.get('ollama', {})
            self.base_url = ollama_config.get('base_url', 'http://localhost:11434')
            self.model_name = ollama_config.get('model', 'qwen2.5:7b')
            self.max_tokens = ollama_config.get('max_tokens', 1000)
            self.temperature = ollama_config.get('temperature', 0.1)
            
            # æµ‹è¯•è¿æ¥
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.model_name in model_names:
                    self.client = 'ollama'  # æ ‡è®°å®¢æˆ·ç«¯ç±»å‹
                    self.logger.info(f"Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
                else:
                    self.logger.warning(f"æ¨¡å‹ {self.model_name} æœªæ‰¾åˆ°ï¼Œå¯ç”¨æ¨¡å‹: {model_names}")
                    self.logger.info(f"è¯·è¿è¡Œ: ollama pull {self.model_name}")
                    self.client = None
            else:
                self.logger.warning("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å¯åŠ¨")
                self.client = None
                
        except ImportError:
            self.logger.error("ç¼ºå°‘requestsåº“ï¼Œè¯·å®‰è£…: pip install requests")
            self.client = None
        except Exception as e:
            self.logger.error(f"Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
            self.client = None
    
    def _init_qwen_client(self):
        """åˆå§‹åŒ–é€šä¹‰åƒé—®å®¢æˆ·ç«¯"""
        try:
            # è·å–é€šä¹‰åƒé—®é…ç½®
            qwen_config = self.llm_config.get('qwen', {})
            api_key = qwen_config.get('api_key')
            
            if not api_key:
                import os
                api_key = os.getenv('DASHSCOPE_API_KEY')
                
            if not api_key:
                self.logger.warning("æœªæ‰¾åˆ°é€šä¹‰åƒé—®APIå¯†é’¥ï¼Œè¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
                self.client = None
                return
            
            # è®¾ç½®å®¢æˆ·ç«¯å‚æ•°
            self.api_key = api_key
            self.model_name = qwen_config.get('model', 'qwen-turbo')
            self.max_tokens = qwen_config.get('max_tokens', 1000)
            self.temperature = qwen_config.get('temperature', 0.1)
            self.client = 'qwen'  # æ ‡è®°å®¢æˆ·ç«¯ç±»å‹
            
            self.logger.info(f"é€šä¹‰åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"é€šä¹‰åƒé—®å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
    
    def _init_glm_client(self):
        """åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯"""
        try:
            # è·å–æ™ºè°±AIé…ç½®
            glm_config = self.llm_config.get('glm', {})
            api_key = glm_config.get('api_key')
            
            if not api_key:
                import os
                api_key = os.getenv('GLM_API_KEY')
                
            if not api_key:
                self.logger.warning("æœªæ‰¾åˆ°æ™ºè°±AI APIå¯†é’¥ï¼Œè¯·è®¾ç½®GLM_API_KEYç¯å¢ƒå˜é‡")
                self.client = None
                return
            
            # è®¾ç½®å®¢æˆ·ç«¯å‚æ•°
            self.api_key = api_key
            self.model_name = glm_config.get('model', 'glm-4-flash')
            self.max_tokens = glm_config.get('max_tokens', 1000)
            self.temperature = glm_config.get('temperature', 0.1)
            self.client = 'glm'  # æ ‡è®°å®¢æˆ·ç«¯ç±»å‹
            
            self.logger.info(f"æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"æ™ºè°±AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
    
    def _init_wenxin_client(self):
        """åˆå§‹åŒ–ç™¾åº¦æ–‡å¿ƒä¸€è¨€å®¢æˆ·ç«¯"""
        try:
            # è·å–æ–‡å¿ƒä¸€è¨€é…ç½®
            wenxin_config = self.llm_config.get('wenxin', {})
            api_key = wenxin_config.get('api_key')
            secret_key = wenxin_config.get('secret_key')
            
            if not api_key or not secret_key:
                import os
                api_key = api_key or os.getenv('WENXIN_API_KEY')
                secret_key = secret_key or os.getenv('WENXIN_SECRET_KEY')
                
            if not api_key or not secret_key:
                self.logger.warning("æœªæ‰¾åˆ°æ–‡å¿ƒä¸€è¨€APIå¯†é’¥ï¼Œè¯·è®¾ç½®WENXIN_API_KEYå’ŒWENXIN_SECRET_KEYç¯å¢ƒå˜é‡")
                self.client = None
                return
            
            # è®¾ç½®å®¢æˆ·ç«¯å‚æ•°
            self.api_key = api_key
            self.secret_key = secret_key
            self.model_name = wenxin_config.get('model', 'ernie-3.5-turbo')
            self.max_tokens = wenxin_config.get('max_tokens', 1000)
            self.temperature = wenxin_config.get('temperature', 0.1)
            self.client = 'wenxin'  # æ ‡è®°å®¢æˆ·ç«¯ç±»å‹
            
            self.logger.info(f"æ–‡å¿ƒä¸€è¨€å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            self.logger.error(f"æ–‡å¿ƒä¸€è¨€å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            
    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤çš„è¯„ä¼°æç¤ºè¯"""
        return """è¯·è¯„ä¼°ä»¥ä¸‹ä¸­è‹±æ–‡ç¿»è¯‘çš„è´¨é‡ï¼Œä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢åˆ†æï¼š
1. å‡†ç¡®æ€§ï¼šç¿»è¯‘æ˜¯å¦å‡†ç¡®ä¼ è¾¾äº†åŸæ–‡æ„æ€
2. æµç•…æ€§ï¼šè¯‘æ–‡æ˜¯å¦è‡ªç„¶æµç•…
3. ä¸€è‡´æ€§ï¼šä¸“ä¸šæœ¯è¯­å’Œå…³é”®è¯æ˜¯å¦ä¸€è‡´
4. å®Œæ•´æ€§ï¼šæ˜¯å¦æœ‰é—æ¼æˆ–æ·»åŠ å†…å®¹

åŸæ–‡ï¼š{source}
è¯‘æ–‡ï¼š{target}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "score": è¯„åˆ†(1-10),
    "accuracy": å‡†ç¡®æ€§è¯„åˆ†(1-10),
    "fluency": æµç•…æ€§è¯„åˆ†(1-10),
    "consistency": ä¸€è‡´æ€§è¯„åˆ†(1-10),
    "completeness": å®Œæ•´æ€§è¯„åˆ†(1-10),
    "issues": ["é—®é¢˜1", "é—®é¢˜2"],
    "suggestions": ["å»ºè®®1", "å»ºè®®2"],
    "explanation": "è¯¦ç»†è¯´æ˜"
}}"""

    def _call_llm(self, prompt: str, max_retries: int = 3) -> Optional[str]:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: æç¤ºè¯
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        if self.client is None:
            return None
            
        for attempt in range(max_retries):
            try:
                if self.provider == 'openai':
                    response = self.client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=self.max_tokens,
                        temperature=self.temperature
                    )
                    return response.choices[0].message.content
                
                elif self.provider == 'ollama':
                    return self._call_ollama(prompt)
                
                elif self.provider == 'qwen':
                    return self._call_qwen(prompt)
                
                elif self.provider == 'glm':
                    return self._call_glm(prompt)
                
                elif self.provider == 'wenxin':
                    return self._call_wenxin(prompt)
                    
            except Exception as e:
                self.logger.warning(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    
        return None
        
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æLLMå“åº”
        
        Args:
            response: LLMå“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„ç»“æœå­—å…¸
        """
        if not response:
            return self._get_error_result("LLMæ— å“åº”")
            
        try:
            # å°è¯•æå–JSONéƒ¨åˆ† - æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
            # ä¼˜å…ˆåŒ¹é…å®Œæ•´çš„JSONå¯¹è±¡
            json_patterns = [
                r'\{[^{}]*"score"[^{}]*\}',  # ç®€å•JSON
                r'\{(?:[^{}]|{[^{}]*})*\}',  # åµŒå¥—JSON
                r'\{.*?\}(?=\s*$|\s*\n|\s*[ã€‚ï¼ï¼Ÿ.])',  # ä»¥æ ‡ç‚¹ç»“å°¾çš„JSON
                r'\{.*\}'  # å…œåº•åŒ¹é…
            ]
            
            json_str = None
            for pattern in json_patterns:
                json_match = re.search(pattern, response, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    try:
                        # å°è¯•è§£æï¼Œå¦‚æœæˆåŠŸå°±ä½¿ç”¨è¿™ä¸ª
                        test_result = json.loads(json_str)
                        if 'score' in test_result:  # ç¡®ä¿åŒ…å«å…³é”®å­—æ®µ
                            break
                    except:
                        continue
            
            if json_str:
                result = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_fields = ['score', 'accuracy', 'fluency', 'consistency', 'completeness']
                missing_fields = []
                invalid_fields = []
                
                for field in required_fields:
                    if field not in result:
                        missing_fields.append(field)
                        result[field] = 5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                        
                # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
                for field in required_fields:
                    if not isinstance(result[field], (int, float)) or not (1 <= result[field] <= 10):
                        invalid_fields.append(f"{field}={result.get(field)}")
                        result[field] = 5
                
                # è®°å½•å¼‚å¸¸æƒ…å†µ
                if missing_fields:
                    self.logger.warning(f"LLMå“åº”ç¼ºå°‘å­—æ®µ: {missing_fields}ï¼Œå·²å¡«å……é»˜è®¤å€¼5åˆ†")
                if invalid_fields:
                    self.logger.warning(f"LLMå“åº”å­—æ®µå€¼æ— æ•ˆ: {invalid_fields}ï¼Œå·²ä¿®æ­£ä¸º5åˆ†")
                    
                # æ·»åŠ è´¨é‡æ ‡è®°
                result['_parse_quality'] = {
                    'missing_fields': missing_fields,
                    'invalid_fields': invalid_fields,
                    'has_issues': len(missing_fields) > 0 or len(invalid_fields) > 0
                }
                        
                # ç¡®ä¿åˆ—è¡¨å­—æ®µå­˜åœ¨
                if 'issues' not in result or not isinstance(result['issues'], list):
                    result['issues'] = []
                if 'suggestions' not in result or not isinstance(result['suggestions'], list):
                    result['suggestions'] = []
                if 'explanation' not in result:
                    result['explanation'] = "æ— è¯¦ç»†è¯´æ˜"
                    
                return result
                
        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æå¤±è´¥: {e}")
            self.logger.error(f"é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.colno}åˆ—")
            self.logger.error(f"åŸå§‹å“åº”å…¨æ–‡:\n{'-'*50}\n{response}\n{'-'*50}")
            if json_str:
                self.logger.error(f"æå–çš„JSONå­—ç¬¦ä¸²:\n{json_str}")
                # å°è¯•æ‰¾å‡ºå…·ä½“çš„é—®é¢˜å­—ç¬¦
                try:
                    problem_char = json_str[e.pos] if e.pos < len(json_str) else '(è¶…å‡ºèŒƒå›´)'
                    self.logger.error(f"é—®é¢˜å­—ç¬¦ä½ç½® {e.pos}: '{problem_char}' (ASCII: {ord(problem_char) if len(problem_char)==1 else 'N/A'})")
                except:
                    pass
            else:
                self.logger.error("æœªèƒ½ä»å“åº”ä¸­æå–åˆ°JSONæ ¼å¼")
                # æ˜¾ç¤ºå“åº”ä¸­æ˜¯å¦åŒ…å«å¤§æ‹¬å·
                if '{' in response and '}' in response:
                    self.logger.error("å“åº”ä¸­åŒ…å«å¤§æ‹¬å·ï¼Œä½†æ­£åˆ™è¡¨è¾¾å¼æœªèƒ½åŒ¹é…")
                else:
                    self.logger.error("å“åº”ä¸­ä¸åŒ…å«JSONå¤§æ‹¬å·")
            
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
        self.logger.warning("JSONè§£æå®Œå…¨å¤±è´¥ï¼Œé™çº§åˆ°æ–‡æœ¬è§£ææ¨¡å¼")
        fallback_result = self._extract_from_text(response)
        
        # æ ‡è®°è¿™æ˜¯é™çº§ç»“æœ
        fallback_result['_parse_quality'] = {
            'missing_fields': ['æ‰€æœ‰å­—æ®µ'],
            'invalid_fields': [],
            'has_issues': True,
            'fallback_mode': True,
            'original_response': response[:500]  # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
        }
        
        return fallback_result
        
    def _extract_from_text(self, text: str) -> Dict[str, Any]:
        """
        ä»çº¯æ–‡æœ¬ä¸­æå–è¯„ä¼°ä¿¡æ¯
        
        Args:
            text: å“åº”æ–‡æœ¬
            
        Returns:
            æå–çš„ç»“æœå­—å…¸
        """
        result = {
            'score': 5,
            'accuracy': 5,
            'fluency': 5,
            'consistency': 5,
            'completeness': 5,
            'issues': [],
            'suggestions': [],
            'explanation': text[:500]  # æˆªå–å‰500å­—ç¬¦ä½œä¸ºè¯´æ˜
        }
        
        # å°è¯•æå–åˆ†æ•°
        score_patterns = [
            r'æ€»åˆ†[ï¼š:]\s*(\d+)',
            r'è¯„åˆ†[ï¼š:]\s*(\d+)',
            r'åˆ†æ•°[ï¼š:]\s*(\d+)',
            r'(\d+)\s*åˆ†'
        ]
        
        for pattern in score_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    score = int(match.group(1))
                    if 1 <= score <= 10:
                        result['score'] = score
                        break
                except ValueError:
                    continue
                    
        return result
        
    def _get_error_result(self, error_msg: str) -> Dict[str, Any]:
        """
        è·å–é”™è¯¯ç»“æœ
        
        Args:
            error_msg: é”™è¯¯ä¿¡æ¯
            
        Returns:
            é”™è¯¯ç»“æœå­—å…¸
        """
        return {
            'score': 0,
            'accuracy': 0,
            'fluency': 0,
            'consistency': 0,
            'completeness': 0,
            'issues': [error_msg],
            'suggestions': [],
            'explanation': f"è¯„ä¼°å¤±è´¥: {error_msg}"
        }
        
    def evaluate_single_pair(self, source: str, target: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªç¿»è¯‘å¯¹
        
        Args:
            source: æºæ–‡æœ¬
            target: ç›®æ ‡æ–‡æœ¬
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        if not source or not target:
            return self._get_error_result("æºæ–‡æœ¬æˆ–ç›®æ ‡æ–‡æœ¬ä¸ºç©º")
            
        if self.client is None:
            return self._get_error_result("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
        # æ„å»ºæç¤ºè¯
        prompt = self.prompt_template.format(source=source, target=target)
        
        # è°ƒç”¨LLM
        response = self._call_llm(prompt)
        
        # è§£æå“åº”
        result = self._parse_llm_response(response)
        
        # æ·»åŠ å…ƒæ•°æ®
        result['llm_provider'] = self.provider
        result['llm_model'] = getattr(self, 'model_name', 'unknown')
        result['raw_response'] = response
        
        return result
        
    def evaluate_dataframe(self, df: pd.DataFrame, source_col: str, target_col: str, 
                          filter_condition: Optional[str] = None) -> pd.DataFrame:
        """
        æ‰¹é‡è¯„ä¼°æ•°æ®æ¡†ä¸­çš„ç¿»è¯‘å¯¹
        
        Args:
            df: æ•°æ®æ¡†
            source_col: æºè¯­è¨€åˆ—å
            target_col: ç›®æ ‡è¯­è¨€åˆ—å
            filter_condition: è¿‡æ»¤æ¡ä»¶ï¼Œåªè¯„ä¼°æ»¡è¶³æ¡ä»¶çš„è¡Œ
            
        Returns:
            åŒ…å«LLMè¯„ä¼°ç»“æœçš„æ•°æ®æ¡†
        """
        if self.client is None:
            self.logger.error("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡LLMè¯„ä¼°")
            result_df = df.copy()
            result_df['llm_score'] = 0
            result_df['llm_evaluation'] = None
            return result_df
            
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filter_condition:
            eval_df = df.query(filter_condition).copy()
            self.logger.info(f"åº”ç”¨è¿‡æ»¤æ¡ä»¶ '{filter_condition}'ï¼Œå¾…è¯„ä¼°æ•°æ®: {len(eval_df)} æ¡")
        else:
            eval_df = df.copy()
            
        self.logger.info(f"ğŸ”„ å¼€å§‹LLMè¯„ä¼°ï¼Œå…± {len(eval_df)} æ¡æ•°æ®")
        
        results = []
        
        # é€ä¸ªè¯„ä¼°ï¼ˆè€ƒè™‘åˆ°APIé™åˆ¶ï¼‰
        for i, (idx, row) in enumerate(tqdm(eval_df.iterrows(), total=len(eval_df), desc="LLMè¯„ä¼°", leave=False)):
            source = str(row[source_col]) if pd.notna(row[source_col]) else ""
            target = str(row[target_col]) if pd.notna(row[target_col]) else ""
            
            self.logger.debug(f"æ­£åœ¨è¯„ä¼°ç¬¬ {i+1}/{len(eval_df)} æ¡æ•°æ®")
            result = self.evaluate_single_pair(source, target)
            results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶ï¼ˆå‡å°‘å»¶è¿Ÿæå‡æ€§èƒ½ï¼‰
            time.sleep(0.1)
        
        self.logger.info(f"âœ“ LLMè¯„ä¼°å®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ•°æ®")
            
        # å°†ç»“æœæ·»åŠ åˆ°æ•°æ®æ¡†
        result_df = df.copy()
        
        # åˆå§‹åŒ–LLMè¯„ä¼°åˆ— - ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
        result_df['llm_score'] = 0.0  # floatç±»å‹
        result_df['llm_accuracy'] = 0.0
        result_df['llm_fluency'] = 0.0
        result_df['llm_consistency'] = 0.0
        result_df['llm_completeness'] = 0.0
        result_df['llm_issues'] = ""  # stringç±»å‹
        result_df['llm_suggestions'] = ""
        result_df['llm_explanation'] = ""
        result_df['llm_evaluation'] = pd.Series([None] * len(result_df), dtype=object)  # æ˜ç¡®æŒ‡å®šobjectç±»å‹
        
        # å¡«å…¥è¯„ä¼°ç»“æœ - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
        for i, (idx, result) in enumerate(zip(eval_df.index, results)):
            try:
                # ç¡®ä¿ç´¢å¼•å­˜åœ¨äºresult_dfä¸­
                if idx not in result_df.index:
                    self.logger.warning(f"ç´¢å¼•{idx}ä¸å­˜åœ¨äºresult_dfä¸­ï¼Œè·³è¿‡")
                    continue
                    
                # å®‰å…¨åœ°å¡«å……æ•°å€¼å­—æ®µ
                result_df.at[idx, 'llm_score'] = float(result.get('score', 0))
                result_df.at[idx, 'llm_accuracy'] = float(result.get('accuracy', 0))
                result_df.at[idx, 'llm_fluency'] = float(result.get('fluency', 0))
                result_df.at[idx, 'llm_consistency'] = float(result.get('consistency', 0))
                result_df.at[idx, 'llm_completeness'] = float(result.get('completeness', 0))
                
                # å®‰å…¨åœ°å¡«å……å­—ç¬¦ä¸²å­—æ®µ
                issues = result.get('issues', [])
                result_df.at[idx, 'llm_issues'] = '; '.join(issues) if isinstance(issues, list) else str(issues)
                
                suggestions = result.get('suggestions', [])
                result_df.at[idx, 'llm_suggestions'] = '; '.join(suggestions) if isinstance(suggestions, list) else str(suggestions)
                
                result_df.at[idx, 'llm_explanation'] = str(result.get('explanation', ''))
                
                # å¡«å……å¤æ‚å¯¹è±¡å­—æ®µ
                result_df.at[idx, 'llm_evaluation'] = result
                
            except Exception as e:
                self.logger.error(f"å¡«å……LLMç»“æœæ—¶å‡ºé”™ï¼Œç´¢å¼•{idx}: {e}")
                self.logger.error(f"DataFrameç´¢å¼•èŒƒå›´: {result_df.index.min()} - {result_df.index.max()}")
                self.logger.error(f"å½“å‰ç´¢å¼•: {idx}, ç±»å‹: {type(idx)}")
                # ä½¿ç”¨é»˜è®¤å€¼
                if idx in result_df.index:
                    result_df.at[idx, 'llm_score'] = 0.0
                    result_df.at[idx, 'llm_accuracy'] = 0.0
                    result_df.at[idx, 'llm_fluency'] = 0.0
                    result_df.at[idx, 'llm_consistency'] = 0.0
                    result_df.at[idx, 'llm_completeness'] = 0.0
                    result_df.at[idx, 'llm_issues'] = "æ•°æ®å¤„ç†é”™è¯¯"
                    result_df.at[idx, 'llm_suggestions'] = ""
                    result_df.at[idx, 'llm_explanation'] = f"å¤„ç†é”™è¯¯: {str(e)}"
                    result_df.at[idx, 'llm_evaluation'] = None
            
        # ç»Ÿè®¡ç»“æœå’Œè´¨é‡åˆ†æ
        evaluated_count = len(results)
        avg_score = sum(r['score'] for r in results) / evaluated_count if evaluated_count > 0 else 0
        
        # ç»Ÿè®¡è§£æè´¨é‡
        parse_issues = 0
        fallback_count = 0
        missing_fields_count = 0
        invalid_fields_count = 0
        
        for result in results:
            if '_parse_quality' in result and result['_parse_quality']['has_issues']:
                parse_issues += 1
                if result['_parse_quality'].get('fallback_mode'):
                    fallback_count += 1
                if result['_parse_quality']['missing_fields']:
                    missing_fields_count += 1
                if result['_parse_quality']['invalid_fields']:
                    invalid_fields_count += 1
        
        self.logger.info(f"LLMè¯„ä¼°å®Œæˆï¼Œè¯„ä¼°äº† {evaluated_count} æ¡æ•°æ®")
        self.logger.info(f"å¹³å‡åˆ†æ•°: {avg_score:.2f}")
        
        # è´¨é‡æŠ¥å‘Š
        if parse_issues > 0:
            self.logger.warning(f"è§£æè´¨é‡æŠ¥å‘Š:")
            self.logger.warning(f"  - æœ‰è§£æé—®é¢˜çš„æ¡ç›®: {parse_issues}/{evaluated_count} ({parse_issues/evaluated_count*100:.1f}%)")
            if fallback_count > 0:
                self.logger.warning(f"  - é™çº§åˆ°æ–‡æœ¬è§£æ: {fallback_count} æ¡")
            if missing_fields_count > 0:
                self.logger.warning(f"  - ç¼ºå°‘å­—æ®µ: {missing_fields_count} æ¡")
            if invalid_fields_count > 0:
                self.logger.warning(f"  - å­—æ®µå€¼æ— æ•ˆ: {invalid_fields_count} æ¡")
            self.logger.warning(f"  å»ºè®®æ£€æŸ¥æç¤ºè¯æ¨¡æ¿æˆ–æ¨¡å‹é…ç½®")
        else:
            self.logger.info("æ‰€æœ‰LLMå“åº”è§£ææ­£å¸¸ âœ“")
        
        return result_df
        
    def get_evaluation_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–è¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            df: åŒ…å«è¯„ä¼°ç»“æœçš„æ•°æ®æ¡†
            
        Returns:
            è¯„ä¼°æ‘˜è¦
        """
        if 'llm_score' not in df.columns:
            return {'error': 'æ•°æ®æ¡†ä¸­æ²¡æœ‰LLMè¯„ä¼°ç»“æœ'}
            
        # è¿‡æ»¤å‡ºæœ‰è¯„ä¼°ç»“æœçš„è¡Œ
        evaluated_df = df[df['llm_score'] > 0]
        
        if len(evaluated_df) == 0:
            return {'error': 'æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ'}
            
        summary = {
            'total_evaluated': len(evaluated_df),
            'average_scores': {
                'overall': evaluated_df['llm_score'].mean(),
                'accuracy': evaluated_df['llm_accuracy'].mean(),
                'fluency': evaluated_df['llm_fluency'].mean(),
                'consistency': evaluated_df['llm_consistency'].mean(),
                'completeness': evaluated_df['llm_completeness'].mean()
            },
            'score_distribution': {
                'excellent (9-10)': len(evaluated_df[evaluated_df['llm_score'] >= 9]),
                'good (7-8)': len(evaluated_df[(evaluated_df['llm_score'] >= 7) & (evaluated_df['llm_score'] < 9)]),
                'fair (5-6)': len(evaluated_df[(evaluated_df['llm_score'] >= 5) & (evaluated_df['llm_score'] < 7)]),
                'poor (1-4)': len(evaluated_df[evaluated_df['llm_score'] < 5])
            },
            'common_issues': self._analyze_common_issues(evaluated_df)
        }
        
        return summary
        
    def _analyze_common_issues(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        åˆ†æå¸¸è§é—®é¢˜
        
        Args:
            df: è¯„ä¼°ç»“æœæ•°æ®æ¡†
            
        Returns:
            å¸¸è§é—®é¢˜åˆ—è¡¨
        """
        issue_counts = {}
        
        for issues_str in df['llm_issues']:
            if pd.notna(issues_str) and issues_str:
                issues = [issue.strip() for issue in str(issues_str).split(';') if issue.strip()]
                for issue in issues:
                    issue_counts[issue] = issue_counts.get(issue, 0) + 1
                    
        # æŒ‰é¢‘ç‡æ’åº
        sorted_issues = sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)
        
        return [{'issue': issue, 'count': count, 'percentage': count/len(df)*100} 
                for issue, count in sorted_issues[:10]]  # è¿”å›å‰10ä¸ªå¸¸è§é—®é¢˜
    
    def _call_ollama(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨Ollama API"""
        try:
            import requests
            
            data = {
                "model": self.model_name,
                "prompt": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚\n\n{prompt}",
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": self.max_tokens
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=120  # å¢åŠ åˆ°120ç§’
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                self.logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"Ollamaè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _call_qwen(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            import requests
            
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.model_name,
                "input": {
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ]
                },
                "parameters": {
                    "max_tokens": self.max_tokens,
                    "temperature": self.temperature
                }
            }
            
            response = requests.post(
                'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation',
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('output') and result['output'].get('choices'):
                    return result['output']['choices'][0]['message']['content']
            else:
                self.logger.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
            return None
            
        except Exception as e:
            self.logger.error(f"é€šä¹‰åƒé—®è°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _call_glm(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨æ™ºè°±AI API"""
        try:
            import requests
            
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature
            }
            
            response = requests.post(
                'https://open.bigmodel.cn/api/paas/v4/chat/completions',
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('choices'):
                    return result['choices'][0]['message']['content']
            else:
                self.logger.error(f"æ™ºè°±AI APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
            return None
            
        except Exception as e:
            self.logger.error(f"æ™ºè°±AIè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def _call_wenxin(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨ç™¾åº¦æ–‡å¿ƒä¸€è¨€API"""
        try:
            import requests
            
            # é¦–å…ˆè·å–access_token
            token_url = "https://aip.baidubce.com/oauth/2.0/token"
            token_params = {
                "grant_type": "client_credentials",
                "client_id": self.api_key,
                "client_secret": self.secret_key
            }
            
            token_response = requests.post(token_url, params=token_params, timeout=30)
            if token_response.status_code != 200:
                self.logger.error("è·å–æ–‡å¿ƒä¸€è¨€access_tokenå¤±è´¥")
                return None
                
            access_token = token_response.json().get('access_token')
            if not access_token:
                self.logger.error("æ–‡å¿ƒä¸€è¨€access_tokenä¸ºç©º")
                return None
            
            # è°ƒç”¨æ–‡å¿ƒä¸€è¨€API
            api_url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/ernie-3.5-turbo"
            headers = {
                'Content-Type': 'application/json'
            }
            
            data = {
                "messages": [
                    {"role": "user", "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚\n\n{prompt}"}
                ],
                "max_output_tokens": self.max_tokens,
                "temperature": self.temperature
            }
            
            response = requests.post(
                f"{api_url}?access_token={access_token}",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('result', '')
            else:
                self.logger.error(f"æ–‡å¿ƒä¸€è¨€APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
            return None
            
        except Exception as e:
            self.logger.error(f"æ–‡å¿ƒä¸€è¨€è°ƒç”¨å¼‚å¸¸: {e}")
            return None
