"""
è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹
ä½¿ç”¨sentence-transformersè®¡ç®—ä¸­è‹±æ–‡æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦
"""

import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Any, Optional
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

from ..utils.config_loader import ConfigLoader
from ..utils.logger import get_logger

# ä¸å¼ºè¡Œä¿®æ”¹sentence_transformersçš„å†…éƒ¨å®ç°


class SimilarityModel:
    """è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        åˆå§‹åŒ–ç›¸ä¼¼åº¦æ¨¡å‹
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = ConfigLoader(config_path)
        self.logger = get_logger(__name__)
        
        # è·å–é…ç½®
        self.similarity_config = self.config.get('detection.similarity', {})
        self.model_name = self.similarity_config.get(
            'model_name', 
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        )
        self.threshold = self.similarity_config.get('threshold', 0.7)
        self.batch_size = self.similarity_config.get('batch_size', 32)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.device = None
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½sentence-transformersæ¨¡å‹"""
        try:
            self.logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            
            # æ£€æµ‹è®¾å¤‡
            if torch.cuda.is_available():
                self.device = 'cuda'
                self.logger.info("ä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                self.device = 'cpu'
                self.logger.info("ä½¿ç”¨CPU")
                
            # åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(self.model_name, device=self.device)
            self.logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.logger.warning("å°†ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
            try:
                # å°è¯•ä½¿ç”¨æ›´å°çš„å¤‡ç”¨æ¨¡å‹
                backup_model = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
                self.model = SentenceTransformer(backup_model, device=self.device)
                self.logger.info(f"å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ: {backup_model}")
            except Exception as e2:
                self.logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•ç›¸ä¼¼åº¦æ¨¡å‹")
                
    def encode_texts(self, texts: List[str], show_progress: bool = True) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            æ–‡æœ¬å‘é‡æ•°ç»„
        """
        if not texts:
            return np.array([])
            
        try:
            # è¿‡æ»¤ç©ºæ–‡æœ¬
            valid_texts = [text if text else "" for text in texts]
            
            # æ‰¹é‡ç¼–ç 
            embeddings = self.model.encode(
                valid_texts,
                batch_size=self.batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=True
            )
            
            return embeddings
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            raise
            
    def calculate_similarity(self, embeddings1: np.ndarray, embeddings2: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—ä¸¤ç»„å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            embeddings1: ç¬¬ä¸€ç»„å‘é‡
            embeddings2: ç¬¬äºŒç»„å‘é‡
            
        Returns:
            ç›¸ä¼¼åº¦æ•°ç»„
        """
        try:
            # ç¡®ä¿å‘é‡æ˜¯äºŒç»´çš„
            if embeddings1.ndim == 1:
                embeddings1 = embeddings1.reshape(1, -1)
            if embeddings2.ndim == 1:
                embeddings2 = embeddings2.reshape(1, -1)
                
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            # å½’ä¸€åŒ–å‘é‡
            norm1 = np.linalg.norm(embeddings1, axis=1, keepdims=True)
            norm2 = np.linalg.norm(embeddings2, axis=1, keepdims=True)
            
            # é¿å…é™¤é›¶
            norm1 = np.where(norm1 == 0, 1, norm1)
            norm2 = np.where(norm2 == 0, 1, norm2)
            
            embeddings1_normalized = embeddings1 / norm1
            embeddings2_normalized = embeddings2 / norm2
            
            # è®¡ç®—ç‚¹ç§¯
            similarities = np.sum(embeddings1_normalized * embeddings2_normalized, axis=1)
            
            return similarities
            
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            raise
            
    def check_similarity_pair(self, source: str, target: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªç¿»è¯‘å¯¹çš„ç›¸ä¼¼åº¦
        
        Args:
            source: æºæ–‡æœ¬
            target: ç›®æ ‡æ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦æ£€æŸ¥ç»“æœ
        """
        if not source or not target:
            return {
                'passed': False,
                'similarity': 0.0,
                'issue': 'empty_text',
                'message': 'æºæ–‡æœ¬æˆ–ç›®æ ‡æ–‡æœ¬ä¸ºç©º'
            }
            
        try:
            # ç¼–ç æ–‡æœ¬
            source_embedding = self.encode_texts([source], show_progress=False)
            target_embedding = self.encode_texts([target], show_progress=False)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_similarity(source_embedding, target_embedding)[0]
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡
            passed = similarity >= self.threshold
            
            if passed:
                message = f'è¯­ä¹‰ç›¸ä¼¼åº¦æ­£å¸¸ ({similarity:.3f})'
                issue = None
            else:
                issue = 'low_similarity'
                
            return {
                'passed': passed,
                'similarity': float(similarity),
                'issue': issue,
                'message': f'ç›¸ä¼¼åº¦è¿‡ä½: {similarity:.3f} < {self.threshold}'
            }
            
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼åº¦æ£€æŸ¥å¤±è´¥: {e}")
            return {
                'passed': False,
                'similarity': 0.0,
                'issue': 'calculation_error',
                'message': f'ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {str(e)}'
            }
            
    def check_dataframe(self, df: pd.DataFrame, source_col: str, target_col: str) -> pd.DataFrame:
        """
        æ‰¹é‡æ£€æŸ¥æ•°æ®æ¡†ä¸­çš„ç¿»è¯‘å¯¹ç›¸ä¼¼åº¦
        
        Args:
            df: æ•°æ®æ¡†
            source_col: æºè¯­è¨€åˆ—å
            target_col: ç›®æ ‡è¯­è¨€åˆ—å
            
        Returns:
            åŒ…å«ç›¸ä¼¼åº¦æ£€æŸ¥ç»“æœçš„æ•°æ®æ¡†
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œå…± {len(df)} æ¡æ•°æ®")
        
        # å‡†å¤‡æ–‡æœ¬æ•°æ®
        source_texts = df[source_col].fillna('').astype(str).tolist()
        target_texts = df[target_col].fillna('').astype(str).tolist()
        
        try:
            # æ‰¹é‡ç¼–ç 
            self.logger.info("ğŸ”„ æ­£åœ¨ç¼–ç æºæ–‡æœ¬...")
            source_embeddings = self.encode_texts(source_texts, show_progress=True)
            self.logger.info("âœ“ æºæ–‡æœ¬ç¼–ç å®Œæˆ")
            
            self.logger.info("ğŸ”„ æ­£åœ¨ç¼–ç ç›®æ ‡æ–‡æœ¬...")
            target_embeddings = self.encode_texts(target_texts, show_progress=True)
            self.logger.info("âœ“ ç›®æ ‡æ–‡æœ¬ç¼–ç å®Œæˆ")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            self.logger.info("ğŸ”„ æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦...")
            similarities = self.calculate_similarity(source_embeddings, target_embeddings)
            self.logger.info("âœ“ ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")
            
            # ç”Ÿæˆç»“æœ
            results = []
            for i, similarity in enumerate(similarities):
                passed = similarity >= self.threshold
                
                if source_texts[i] == '' or target_texts[i] == '':
                    issue = 'empty_text'
                    message = 'æºæ–‡æœ¬æˆ–ç›®æ ‡æ–‡æœ¬ä¸ºç©º'
                    passed = False
                elif passed:
                    issue = None
                    message = f'è¯­ä¹‰ç›¸ä¼¼åº¦æ­£å¸¸ ({similarity:.3f})'
                else:
                    issue = 'low_similarity'
                    message = f'è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡ä½ ({similarity:.3f} < {self.threshold})'
                    
                results.append({
                    'passed': passed,
                    'similarity': float(similarity),
                    'issue': issue,
                    'message': message
                })
                
            # æ·»åŠ ç»“æœåˆ°æ•°æ®æ¡†
            result_df = df.copy()
            result_df['similarity_score'] = [r['similarity'] for r in results]
            result_df['similarity_passed'] = [r['passed'] for r in results]
            result_df['similarity_issue'] = [r['issue'] for r in results]
            result_df['similarity_message'] = [r['message'] for r in results]
            
            # ç»Ÿè®¡ç»“æœ
            passed_count = sum(r['passed'] for r in results)
            avg_similarity = np.mean([r['similarity'] for r in results])
            
            self.logger.info(f"ç›¸ä¼¼åº¦æ£€æŸ¥å®Œæˆ")
            self.logger.info(f"é€šè¿‡ç‡: {passed_count}/{len(results)} ({passed_count/len(results)*100:.1f}%)")
            self.logger.info(f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            
            return result_df
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡ç›¸ä¼¼åº¦æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›å¸¦æœ‰é”™è¯¯ä¿¡æ¯çš„æ•°æ®æ¡†
            result_df = df.copy()
            result_df['similarity_score'] = 0.0
            result_df['similarity_passed'] = False
            result_df['similarity_issue'] = 'calculation_error'
            result_df['similarity_message'] = f'ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {str(e)}'
            return result_df
            
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if self.model is None:
            return {'status': 'not_loaded'}
            
        return {
            'status': 'loaded',
            'model_name': self.model_name,
            'device': self.device,
            'threshold': self.threshold,
            'batch_size': self.batch_size,
            'max_seq_length': getattr(self.model, 'max_seq_length', 'unknown')
        }
        
    def update_threshold(self, new_threshold: float):
        """
        æ›´æ–°ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Args:
            new_threshold: æ–°çš„é˜ˆå€¼
        """
        if 0 <= new_threshold <= 1:
            self.threshold = new_threshold
            self.logger.info(f"ç›¸ä¼¼åº¦é˜ˆå€¼å·²æ›´æ–°ä¸º: {new_threshold}")
        else:
            raise ValueError("é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
            
    def benchmark_model(self, test_pairs: List[Tuple[str, str]]) -> Dict[str, Any]:
        """
        å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            test_pairs: æµ‹è¯•æ–‡æœ¬å¯¹åˆ—è¡¨
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        if not test_pairs:
            return {'error': 'æ— æµ‹è¯•æ•°æ®'}
            
        self.logger.info(f"å¼€å§‹åŸºå‡†æµ‹è¯•ï¼Œå…± {len(test_pairs)} ä¸ªæµ‹è¯•å¯¹")
        
        similarities = []
        processing_times = []
        
        import time
        
        for source, target in test_pairs:
            start_time = time.time()
            result = self.check_similarity_pair(source, target)
            end_time = time.time()
            
            similarities.append(result['similarity'])
            processing_times.append(end_time - start_time)
            
        return {
            'test_count': len(test_pairs),
            'avg_similarity': np.mean(similarities),
            'min_similarity': np.min(similarities),
            'max_similarity': np.max(similarities),
            'avg_processing_time': np.mean(processing_times),
            'total_time': sum(processing_times)
        }
